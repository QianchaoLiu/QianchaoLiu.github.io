---
layout: post
title:  "神经网络定义及实现"
date:   2016-02-17 10:00:04
categories: liuqianchao update
---

&nbsp;&nbsp;&nbsp;&nbsp;就像CNN(Convolutional Neural Network)在图像特征提取发挥着有效作用一样，RNN(Recurrent Neural Network)在自然语言领域同样展现了显著的有效性。在这篇文章中，将介绍并训练一个CNN模型，之后将介绍RNN和LSTM.


### 1.Convolutional Neural Network  

<div align="center">
<img src="{{ site.url }}/assets/Conv2-9x5-Conv2Conv2.png" width="500" height="310">
</div>

&nbsp;&nbsp;&nbsp;&nbsp;从仿生学的角度思考，我们获取图像时，是由一系列的神经元网络状连接而成，从串联的角度是由一系列由低级神经元（获取图像基本的像素信息）到高级神经元（抽象能力逐步增强），从并联的角度，在低级神经元层，横向来看又有无数低级神经元连接而成，这些神经元负责不同的图像采集区域。由这些低级神经元和高级神经元共同组成cnn的conv layer。而从“视网膜”，到“中枢组织”再到“大脑的视觉皮层（又分为初级高级）”相当于由若干的conv layer，这样就组成了信息逐层处理的cnn。


### 2.Recurrent Neural Network  



<div align="center">
<img src="{{ site.url }}/assets/RNN-unrolled.png" width="470" height="130">
</div>
&nbsp;&nbsp;&nbsp;&nbsp;无论是卷积神经网络(cnn)还是全连接神经网络(fully connected network), forward过程中，隐层中每一层的输入都是仅来自上一层，信息是逐层传递的，但我们体验文本阅读时，上下文的关联相比与图像识别的需求更高了。我们理解一个词的含义时，不仅不要这个词本身(一个词本身也是具有许多种含义的)，同时需要来自上下文的支持，因此修正之前神经网络模型的仅从上一层获取输入，增加本层间信息流的输入，就产生了循环神经网络(recurrent nn, 后文中rnn特指recurrent nn, 而非recursive nn),接下来我将介绍一种特殊的RNN：LSTM.

### 3.Long Short Term Memory   
&nbsp;&nbsp;&nbsp;&nbsp;一方面，较长的语句中，前后句间的word间隔过远，信息保留较少，比如：“云在‘天空’中”，‘天空’这个词很容易通过‘云’这个词来锁定，RNN对该情形也能较好地覆盖；但“我生活在法国，所以说一口流利的’法语‘”中，法语与法国间隔较远，存在明显的gap，如果关联词间的gap较大，RNN已经不能有效地关联相关信息.所以RNN只能较好地处理“short term”的情形。   
&nbsp;&nbsp;&nbsp;&nbsp;另一方面,由于RNN自身结构的原因(大量的weights),较小的weights经常造成梯度弥散现象(gradient vanishing),梯度弥散现象进一步加剧了‘long term memory’的难度，较大的weights则会造成gradient exploding。


<div align="center">
<img src="{{ site.url }}/assets/LSTM3-chain.png" width="600" height="240">
</div>
&nbsp;&nbsp;&nbsp;&nbsp;这里于是引入LSTM的概念，首先，从层内的横向角度来看，区别于RNN中横向传来的一个$$h_{t-1}$$，这里增加一个横向传递的变量cell state: $$C_{t-1}$$，用以“线性”地保留上文信息(在传统RNN中$$wh_{t-1}*h_{t-1}＋ wx_{t}*x_{t}$$经过一个激活函数的非线性变换为$$h_{t}$$)。

<div align="center">
<img src="/assets/LSTM3-C-line.png" width="600" height="180">
</div>
&nbsp;&nbsp;&nbsp;&nbsp;接下来是gates的概念，通过设置引入激活函数(sigmoid)来建立“门”，从而决定是否保留该项信息。每一个图中的黄色方块都是一个完整的神经元，包括weight，输入以及激活函数。激活函数(取$$\sigma(wh_{t-1}*h_{t-1}＋ wx_{t}*x_{t})$$)的结果为1时，$$C_{t-1}*1$$运算的结果即为保留之前的信息。


<div align="center">
<img src="{{ site.url }}/assets/LSTM3-gate.png" width="80" height="110">
</div>
&nbsp;&nbsp;&nbsp;&nbsp;下图所示的第一个gate，是用来决定之前的信息是否舍弃的。

<div align="center">
<img src="{{ site.url }}/assets/LSTM3-focus-f.png" width="580" height="180">
</div>

&nbsp;&nbsp;&nbsp;&nbsp;接下来我们来决定什么新信息要被加入到$$C_{t}$$中，经过$$+$$运算$$C_{t-1}$$就变成了$$C_{t}$$ ($$C_{t}=f_{t}*C_{t-1}+i_{t}*\hat{C}{t}$$)

<div align="center">
<img src="{{ site.url }}/assets/LSTM3-focus-i.png" width="580" height="180">
</div>

&nbsp;&nbsp;&nbsp;&nbsp;剩下要输出的便是我们传递给下一隐层和同一层下一个LSTM单元的输入$$h_{t}$$

<div align="center">
<img src="{{ site.url }}/assets/LSTM3-focus-o.png" width="580" height="180">
</div>


{{ site.url }}


问题:RNN 是否考虑的下文的内容？



### 3. Implement of LSTM   



#### Reference
1. Christopher Olah [colah's blog](http://colah.github.io/posts/2015-08-Understanding-LSTMs/)


