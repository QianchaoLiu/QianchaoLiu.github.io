---
layout: post
title:  "NLP with Neural Network"
date:   2016-6-6 10:00:04
categories: liuqianchao update
---

&nbsp;&nbsp;&nbsp;&nbsp;本文是使用神经网络的方法进行自然语言处理的技术路线，以Stanford的CS224d课程为主，仅供参考。

### 1.一些必要的知识   
&nbsp;&nbsp;&nbsp;&nbsp;1.**线性代数**，包括矩阵的运算，以及模，秩，特征值、特征向量以及和导数求解相关的矩阵知识(比如牛顿法中的Hessian Matrix)等。   
&nbsp;&nbsp;&nbsp;&nbsp;2.**概率论**，包括概率密度函数，条件概率，联合概率，贝叶斯，协方差等。   
&nbsp;&nbsp;&nbsp;&nbsp;3.**凸优化**，包括什么是凸集合，凸函数，如何进行。   


### 2.课程正文

### 2.1词向量   

&nbsp;&nbsp;&nbsp;&nbsp;本小节的重点是word vectors，介绍了词汇向量化的几种经典的方法。

&nbsp;&nbsp;&nbsp;&nbsp;首先介绍了使用descrete representation(比如one-hot)的方法进行词汇向量化的方法，其中一个典型的代表就是使用WordNet这样的**字典**,但是直接使用字典的方式来将词汇向量化，有很多问题存在：   
1. 会丢失很多词汇的本意细节(nuance), 比如一词多义的问题没办法保留多个意思。  
2. wordnet中的词汇有限，不能及时覆盖所有的词汇。   
3. 不能直接用来计算词汇之间的相似性。   

&nbsp;&nbsp;&nbsp;&nbsp;第二种表示方式是使用neighbors的count来表示词汇，其思想是**一个词可以用句子中它周围的词来表示**。该种方法有两种的选择，一个是使用“full document”，另一个种是使用“windows”；前者的思想又引出了latent semantic analysis(可以用于文章分类等)；使用neighbors来进行词汇的表达存在的问题包括：   
1. 需要维护高维矩阵，该矩阵需要大量的存储空间   
2. 模型的鲁棒性不高     
&nbsp;&nbsp;&nbsp;&nbsp;由此，针对上述问题，提出的解决方式是进行矩阵降维，常用的方法包括：SVD（对于$$m n$$矩阵，时间复杂度为$$o(mn^{2})$$，其中$$n< m$$）,此外SVD分解的方法实现词汇向量化对于新词汇的加入不是很友好。      

&nbsp;&nbsp;&nbsp;&nbsp;除了第二种方法中的使用临近词汇的count来表示，并通过一定的降维方式来实现word embedding外，还有一种方式是**直接生成低维向量**(其实这个低维一般也有几百维)。该种方法的思想是通过训练机器学习模型来“modeling”词向量,其代表方法包括Google的word2vec，其特点是对于新的词汇有较好的接收。该种方法通过建立目标函数，使用梯度下降(链式法则)的方法来maximize/minimize目标函数。   
&nbsp;&nbsp;&nbsp;&nbsp;其基本步骤包括：   
1. 定义目标函数/Cost   
2. small random numbers初始化参数（对于Word2vec 参数为2*V*d个数，其中V为字典长度，d为每个word的词向量维度,这里的2是因为对于每一个词有v,u两个向量来表示,一般使用词向量时，v+u作为结果）   
3. 对Cost进行SGD(Updating parameters with each window in Word2Vec)   
&nbsp;&nbsp;&nbsp;&nbsp;针对Word2vec这种方法可以展开思考一些问题，比如每个window的词比较少，这样梯度是稀疏的(参数中只有2c+1个参数需要更新，w为window长)，为解决该问题，只处理(更新)目标2c+1个参数(by only update certain columns of full embedding matrix U(left or right words) and V(center word))。      

&nbsp;&nbsp;&nbsp;&nbsp;下面给出Word2Vec的目标函数的子项(每一个窗口内的neighbor词$$o$$均对应下面给的exponent概率)：  

<div align="center">$$p(o|c)=\frac{exp(v_{c}^{T}u_{o})}{\sum_{w=1}^W exp(v_{c}^{T} u_{w})}$$  </div> 

&nbsp;&nbsp;&nbsp;&nbsp;In above formula, we could find that we need to sum all the exponent function of neighbor words, which is computationally expensive. Hence, we change the objective function.   

<div align="center">$$J(\theta)=log \sigma(u_{o}^{T}v_{c}) + \sum_{t=1}^k E_{j \~ P(w)}[log \sigma(-u_{j}^{T}v_{c})]$$  </div>

&nbsp;&nbsp;&nbsp;&nbsp;改成上式后的方法，叫做Negative Sampling, 其中$$k$$可以取10，20这样的数；$$\sigma$$是sigmoid function。$$\sigma(-x)=1-\sigma(x)$$；Rondom函数有特定的设计，corpus中频繁出现的词，更有可能被选出来，但是这个可能性要低于它在corpus中出现的频率。我们认为Negative Sampling相比于之前的目标函数，增加了额外的信息：使不相关的(random)的词汇距离尽可能远。   

### 2.2 Word window classification and nerual network   
&nbsp;&nbsp;&nbsp;&nbsp;分类，在自然语言中的用处包括：从词到词的预测(比如说补全句子)，情感分析，命名实体识别（named entity），分析决策（sell/buy decision），翻译等。下面将给出分类的相关内容。    
&nbsp;&nbsp;&nbsp;&nbsp;**LR**,Logistic Regression(之前blog里介绍过的SVM只会给出分类的结果，而LG会给出分类的概率，比如分为正类的可能性)，LG是使用sigmoid函数，相当于线性回归之后加入了Logistic变换。   

&nbsp;&nbsp;&nbsp;&nbsp;NLP的Classification与传统意义上的classification的不同在于，NLP中除了训练模型的参数外，还可以把词向量加入到需要训练的参数中，并且，当训练集非常大时，同时进行词向量的训练(update)，效果较好。   
然后引入Window classificaition,一般对单个word的分类，在实际场景中很少用到，此外，word在句子中包含的信息会更多，因此，我们对NLP进行分类时，一般是指window classification。   

&nbsp;&nbsp;&nbsp;&nbsp;单层神经网络(one hidden layer nn/three layers nn)，对于windows classification问题，比如其中一个sentence窗口是"Museums in Pairs are amazing"，用$$x$$来表示,且$$x \in \mathbb{R}^{20 \times 1}$$，然后根据我们设计的隐层metric（结点的数目），可以假设$$W \in \mathbb{R}^{8 \times 20}$$,由隐层到输出层的映射$$U \in \mathbb{R}^{8 \times 1}$$,可得$$s = U^{T} \times f(Wx+b) \in \mathbb{R}$$,s即为分类的得分(score)。   
&nbsp;&nbsp;&nbsp;&nbsp;如果说"Museums in Pairs are amazing"是True window,那么可以定义一些corrupt windows,比如(not all museums in Paris),通过训练，使true windows的得分$$S$$高，corrupt windows得分$$S_{c}$$低。比如可以将目标函数定义为：

<div align="center">$$minimize J,  J = max(0,1-S+S_{c})$$  </div>

&nbsp;&nbsp;&nbsp;&nbsp;该目标函数叫做Max-Margin Objective function,在实际操作中一般忽略左侧的max和0。对于每一个true window，都需要sample几个corrupt windows。   
&nbsp;&nbsp;&nbsp;&nbsp;这里尝试推导$$W_{ij}$$的梯度：

<div align="center">$$\frac{\Delta S}{\Delta W_{ij}} = U_{i} \frac{\Delta f(Wx+b)}{\Delta W_{ij}} = U_{i} \frac{f^{'}(Wx+b) \Delta(Wx+b)}{\Delta W_{ij}} = U_{i} f^{'}(Wx+b) x_{j} = U_{i}f^{'}(z_{i})x_{j}= \delta _{i}x_{j}$$    </div>

&nbsp;&nbsp;&nbsp;&nbsp;当$$f(z)$$为sigmoid function时，$$f^{'}(z) = 1 - f(z)$$,且有$$z_{i} = w_{i}x+b_{i} = \sum_{j=1}^n W_{ij}x_{j} + b_{i} $$,其中$$\delta _{i}$$称为local error signal，$$x_{j}$$称为local input signal    

&nbsp;&nbsp;&nbsp;&nbsp;接下来介绍第一个tip，反向传播计算时的参数复用，$$\frac{\Delta S}{\Delta W_{ij}} = U_{i}f^{'}(z_{i})x_{j}=U_{i}(1-f(z_{i}))x_{j}$$,作为activation $$a$$的error message (responsibility). 这样就可以省去计算了；同理可以求得biases $$b$$，可知$$b$$是完全不增加新的计算量的。   

&nbsp;&nbsp;&nbsp;&nbsp;第二个tip，Multi-task learning/Weight sharing。其思想是可以通过参数共享来同时训练多个tasks，比如NLP中的词性标注POS(Part-of-Speech) ，NER（Named Entity Recognition）可以同时进行，两个模型在隐层使用同样的weights，但在output层使用不同的weights参数。      

&nbsp;&nbsp;&nbsp;&nbsp;接下来介绍了一些训练神经网络的细节：

&nbsp;&nbsp;&nbsp;&nbsp;1. 激活函数：1）$$logistic$$(sigmoid)，$$f^{'}(z)=f(z)(1-f(z))$$；   
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;2）$$tanh$$:$$\frac{e^{z}-e^{-z}}{e^{z}+e^{-z}}$$,并且$$tanh(z)=2logistic(2z)-1$$，$$f^{'}(z)=1-f(z)^{2}$$   
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;3) $$ReLu$$，$$max(0,x)$$   

&nbsp;&nbsp;&nbsp;&nbsp;2. Gradient check，作为神经网络中的“debug”，gradient check可以帮助我们确定梯度计算的准确性。梯度的计算是通过pen-and-paper（推导出来的导数式），而check的时候，通过数值近似的方法来计算：$$f^{'}(\theta)=\frac{J(\theta + \epsilon)+J(\theta - \epsilon)}{2\epsilon}$$，其中$$\epsilon$$非常小，为$$10^{-4}$$数量级，然后比较两个计算结果，确定其相差不多。

&nbsp;&nbsp;&nbsp;&nbsp;3. Model simplification, start from simplest model to find bugs.

&nbsp;&nbsp;&nbsp;&nbsp;4. Parameter Initialization, 假设$$W_{ij} \in [-1,1]$$，$$x_{j} \in [-100,100]$$，并且$$b_{i} \in [-100,100]$$

#### Reference
1. Stanford [cs224d](http://cs224d.stanford.edu/index.html)

