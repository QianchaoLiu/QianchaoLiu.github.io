---
layout: post
title:  "LDA"
date:   2016-10-5 10:00:04
categories: liuqianchao update
---


### What is LDA?   

&emsp; LDA一般被认为是两个概念的缩写，Linear Discriminant Analysis（线性判别分析）和Latent Dirichlet Allocation，本文讨论的内容是后者。   
&emsp; 在了解LDA的原理之前，首先我们要看LDA是用来做什么的：假设我们有多个Document，同时指定要获取的Topic的数目，LDA会告诉我们每个Document归属于某个Topic的概率。根据这功能，我们其实是可以将文本按照topic进行聚类；

### How it works?

&emsp; LDA对于document的形成是有如下的假设的：

- 根据Poisson分布，选择这篇document的字数。

- 每篇文章都是包含多个topics的，比如我写这篇blog时，选择的topic可能是LDA占60%，统计方法占20%，代码应用占20%。   
- 接下来对于每个我在document中输入的词，词首先依概率归属于(LDA, 统计方法，代码应用) ，比如60%可能性选择到LDA。   
- 对于我即将输入的这个属于LDA topic的词，它又有一定的概率分布（比如用词a可能性10%，b为15%...）。

&emsp; 接下来，有了以上的假设。当我们使用LDA，来对一些documents进行分析时，假定我们选择了数目为K的topics，我们想要得到每篇document分别属于上述K的topics的概率，以及每个topic下的words分布。LDA分析实现的步骤如下：

- 遍历所有的documents，随机的选择each word in document归属于某个topic，这样我们就得到每篇document的topic distribution和每个topic的words distribution.

- 接下来我们再次进行遍历（遍历每篇document，每篇document的每个word），来改善上述的random的分布。首先定义：

  - $$P(topic \ t  \ \| \ document \ d) = \frac{num \ of \ words \ belong \ to \ topic \ t}{num \ of \ words \ in \ document \ d}$$，document下某topic的概率等于该document下属于该topic的词汇的出现概率

  - $$P(word \ w  \ \| \ topic \ t) = \frac{num \ of \ word \ w}{num \ of \ words \ belong \ to \ topic \ t}$$， 即我们想到的第二个分布,这里是over all documents，属于topic t的word w的数目/所有属于topic t的word的出现总数。     

  用公式来表示$$P(word \ w  \ \| \ document \ d) = P(word \ w  \ \| \ topic \ t)*P(topic \ t  \ \| \ document \ d)$$。这样对于词w，在不同的topic下都可以计算出它的$$P(word \ w  \ \| \ document \ d)$$。这里我们设定一些策略来选择新的topic，可能的策略包括最大化$$P(word \ w  \ \| \ document \ d)$$，实际比较常用的策略为Gibbs Sampling。

- 重复上述，直到达到一个steady state（convergence）。

#### Reference
1. Websites [quora](https://www.quora.com/What-is-a-good-explanation-of-Latent-Dirichlet-Allocation)

