---
layout: post
title:  "Support Vector Machine"
date:   2015-11-28 22:35:04
categories: liuqianchao update
---

Support Vector Machine(*SVM*), k-Nearest Neighbors algorithm(*k-NN*), and Naive Bayes classifier(*NB*) are the most popular and basic approaches for classification. In this article, I will give some mathmetical equations of *SVM* and approaches to use *SVM*.

###1. What is *SVM*?
In general, there are two kinds of classificaiton, **non-parametric** and **parametric** approaches. The parametric approach is to assume a simple parametric model for density functions and to estimate the parameters of the model using an available training set.
However, for most of cases, we can not assume that the desity of data samples is characterised by a series of parameters in this irregular world. So we introduce the non-parametric method which includes *SVM*.

###2. How it works?

###3. Using *SVM* in your work.
There is a package named *scikit-learn* in Python. Find information to install it at [here](http://scikit-learn.org/stable/index.html). We can use the official date sets of *scikit-learn*, and examine the performance of classification. Here, we use *Iris* dataset. There are 3 kinds of iris, each sample has 4 properties which include petal length, petal width, sepal length, and sepal width.    

In this experiment, we removed one kind of iris, and two groups of iris are left. After this process, we have two kinds of iris, and each kind has 50 samples. And in order to draw two-dimensional chart, two properties are left: petal length and sepal length.

{% highlight python %}
#acquire and handle dataset.
from sklearn import datasets 
iris=datasets.load_iris()
dateset_property=[]
dateset_target=[]
for num in range(len(iris.target)):
    if iris.target[num]!=2: #iris.target[num]=2 is corresponding to the third kind of iris
        dateset_property.append(iris.data[num])
        dateset_target.append(iris.target[num])
{% endhighlight %}   

{% highlight python %}
import numpy as np
from sklearn import svm
from sklearn.cross_validation import train_test_split
import matplotlib.pyplot as plt

x = np.array(dateset_property)
y = np.array(dateset_target)

x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.0)

h = .02
# create a mesh to plot in
x_min, x_max = x_train[:, 0].min() - 0.1, x_train[:, 0].max() + 0.1
y_min, y_max = x_train[:, 1].min() - 1, x_train[:, 1].max() + 1
xx, yy = np.meshgrid(np.arange(x_min, x_max, h),
                     np.arange(y_min, y_max, h))

# title for the plots
titles = ['LinearSVC (linear kernel)',
          'SVC with polynomial (degree 3) kernel',
          'SVC with RBF kernel',
          'SVC with Sigmoid kernel']
clf_linear  = svm.SVC(kernel='linear').fit(x, y)
#clf_linear  = svm.LinearSVC().fit(x, y)
clf_poly    = svm.SVC(kernel='poly', degree=3).fit(x, y)
clf_rbf     = svm.SVC().fit(x, y)
clf_sigmoid = svm.SVC(kernel='sigmoid').fit(x, y)

for i, clf in enumerate((clf_linear, clf_poly, clf_rbf, clf_sigmoid)):
    answer = clf.predict(np.c_[xx.ravel(), yy.ravel()])
    print(clf)
    print(np.mean( answer == y_train))
    print(answer)
    print(y_train)

    plt.subplot(2, 2, i + 1)
    plt.subplots_adjust(wspace=0.4, hspace=0.4)

    # Put the result into a color plot
    z = answer.reshape(xx.shape)
    plt.contourf(xx, yy, z, cmap=plt.cm.Paired, alpha=0.8)

    # Plot also the training points
    plt.scatter(x_train[:, 0], x_train[:, 1], c=y_train, cmap=plt.cm.Paired)
    plt.xlabel(u'petal length')
    plt.ylabel(u'sepal length')
    plt.xlim(xx.min(), xx.max())
    plt.ylim(yy.min(), yy.max())
    plt.xticks(())
    plt.yticks(())
    plt.title(titles[i])

plt.show()
{% endhighlight %} 
#### Result of classification
![img]({{site.url}}assets/svm.png)

#### Reference
1. Andrew R. Webb and Keith D. Copsey *Statistical Pattern Recognition(Third Edition)*.
