---
layout: post
title:  "Support Vector Machine"
date:   2015-11-28 22:35:04
categories: liuqianchao update
---

Support Vector Machine(*SVM*), k-Nearest Neighbors algorithm(*k-NN*), and Naive Bayes classifier(*NB*) are the most popular and basic approaches for classification. In this article, I will give some mathmetical equations of *SVM* and approaches to use *SVM*.

###1. What is *SVM*?
In general, there are two kinds of classificaiton, **non-parametric** and **parametric** approaches. The parametric approach is to assume a simple parametric model for density functions and to estimate the parameters of the model using an available training set.
However, for most of cases, we can not assume that the desity of data samples can be characterised by a series of parameters in this irregular world. So we introduce the non-parametric method which includes *SVM*.

###2. How it works?
The *SVM* uses a very simple idea. Beforing explaining how *SVM* works, we will introduce the concept of ***kernel trick***. How can we distinguish ◇ from △ showed below? Obiviously, there are no hyperplane to classify one from the other. To solve this problem, *kernel trick* has been created.
 

<div align="center">   
<canvas id="myCanvas" width="250" height="180" style="border:0px solid #c3c3c3;">
Your browser does not support the canvas element. here is graph written by html.
</canvas>
<script type="text/javascript">
var c=document.getElementById("myCanvas");
var ctx=c.getContext("2d");

var txt1="△                     ◇";
var txt2="◇                     △";
ctx.font = "20px Helvetica red";            
ctx.textBaseline = 'top';
ctx.fillText(txt1, 40, 0);
ctx.fillText(txt2, 0,130);
</script>
</div>
Suppose that we have a set of training patterns.{$$x_{i},i=1,...,m$$} assigned to one of two classes $$\omega_{1}$$ and $$\omega_{2}$$, with corresponding labels $$y_{i}=\pm1$$. 
Denote the linear discriminant function $$g(x)$$, and $$x$$ is a n-dimension vector:

<div align="center">
$$g(x)=w^{T}x+w_{0}$$
</div>
if $$g(x)<0$$ we see $$x$$ as label $$y_{i}=-1$$, and if $$g(x)>0$$ we see $$x$$ as label $$y_{i}=+1$$.
Here we 


###3. Using *SVM* in your work.
There is a package named ***scikit-learn*** in Python. Find information to install it at [here](http://scikit-learn.org/stable/index.html). We can use the official date sets of *scikit-learn*, and examine the performance of classification. Here, we use *Iris* dataset. There are 3 kinds of iris, each sample has 4 properties which include petal length, petal width, sepal length, and sepal width.    

In this experiment, we will show how to conduct a binary classification task. We removed one kind of iris, and two groups of iris are left. After this process, we have two kinds of iris, and each kind has 50 samples. And in order to draw two-dimensional chart, two properties are left: petal length and sepal length.

####3.1 Handle the dataset
{% highlight python %}
#acquire and handle dataset.
from sklearn import datasets 
iris=datasets.load_iris()
dateset_property=[]
dateset_target=[]
for num in range(len(iris.target)):
    if iris.target[num]!=2: #iris.target[num]=2 is corresponding to the third kind of iris
        dateset_property.append(iris.data[num])
        dateset_target.append(iris.target[num])
x = np.array(dateset_property)
y = np.array(dateset_target)
{% endhighlight %}   

####3.2 Train the model.
In this experiment, I apply 4 kinds of kernel function, including *linear*,*poly*,*RBF* and *sigmoid*.
{% highlight python %}
import numpy as np
from sklearn import svm
from sklearn.cross_validation import train_test_split

#split the dataset, and rearrange the list
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.0)

# create a mesh to plot in
x_min, x_max = x_train[:, 0].min() - 0.1, x_train[:, 0].max() + 0.1
y_min, y_max = x_train[:, 1].min() - 1, x_train[:, 1].max() + 1
xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.02), np.arange(y_min, y_max, 0.02))

# title for the plots
titles = ['LinearSVC (linear kernel)','SVC with polynomial (degree 3) kernel', 'SVC with RBF kernel', 'SVC with Sigmoid kernel']
clf_linear  = svm.SVC(kernel='linear').fit(x_train, y_train)
clf_poly    = svm.SVC(kernel='poly', degree=3).fit(x_train, y_train)
clf_rbf     = svm.SVC().fit(x_train, y_train)
clf_sigmoid = svm.SVC(kernel='sigmoid').fit(x_train, y_train)
{% endhighlight %} 

####3.3 Draw plot
{% highlight python %}
import matplotlib.pyplot as plt
for i, clf in enumerate((clf_linear, clf_poly, clf_rbf, clf_sigmoid)):
    #result are the predict result of all the meshgrid.
    result = clf.predict(np.c_[xx.ravel(), yy.ravel()])

    #set the composition of plot
    plt.subplot(2, 2, i + 1)
    plt.subplots_adjust(wspace=0.35, hspace=0.35)

    # Put the result into a color plot.
    z = result.reshape(xx.shape)
    plt.contourf(xx, yy, z, cmap='terrain', alpha=0.4)

    # Plot the training points
    plt.scatter(x_train[:, 0], x_train[:, 1], c=y_train, cmap='terrain',alpha=0.9)
    plt.xlabel(u'petal length')
    plt.ylabel(u'sepal length')
    plt.xlim(xx.min(), xx.max())
    plt.ylim(yy.min(), yy.max())
    plt.title(titles[i])
plt.show()
{% endhighlight %} 

#### Result of classification
<div align="center">
<img src="{{ site.url }}/assets/screenshot.png" width="550" height="440">
</div>
<div align="center">
Figure: result of classification
</div>

#### Reference
1. Andrew R. Webb and Keith D. Copsey *Statistical Pattern Recognition(Third Edition)*.
